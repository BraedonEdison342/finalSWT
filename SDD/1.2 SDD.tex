\documentclass[15pt]{article}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{enumitem}

\pagestyle{fancy}
\fancyhf{}
\rhead{Software Design Document}
\lhead{Titanic Space Ship AI Model }
\rfoot{\thepage}

\title{Software Design Document (SDD)\\\large Project Name}
\author{Team Name or Author(s)}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction}
\subsection{Purpose}
Briefly describe the purpose of this document and the software being developed.

\subsection{Scope}
Overview of what the software will do. Mention primary functionality.

\subsection{Definitions, Acronyms, and Abbreviations}
List relevant definitions and terms.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|l|p{8cm}|}
    \hline
    \textbf{Acronym} & \textbf{Full Form} & \textbf{Description} \\
    \hline
    AI       & Artificial Intelligence            & Systems that simulate human intelligence in machines. \\
    ML       & Machine Learning                   & A subset of AI that enables models to learn from data. \\
    CSV      & Comma-Separated Values             & Common format for structured tabular data files. \\
    EDA      & Exploratory Data Analysis          & The process of visually and statistically analyzing data. \\
    N/A      & Not Available / Not Applicable     & Often used to denote missing or undefined values. \\
    ID       & Identifier                         & A unique label used to distinguish data records. \\
    ROC      & Receiver Operating Characteristic & A plot used to assess binary classifier performance. \\
    AUC      & Area Under the Curve               & Metric summarizing the ROC curve. \\
    TP       & True Positive                      & A correct prediction that the passenger was transported. \\
    FP       & False Positive                     & An incorrect prediction that the passenger was transported. \\
    TN       & True Negative                      & A correct prediction that the passenger was not transported. \\
    FN       & False Negative                     & An incorrect prediction that the passenger was not transported. \\
    XGBoost  & Extreme Gradient Boosting          & A powerful tree-based machine learning algorithm. \\
    SVM      & Support Vector Machine             & A machine learning algorithm for classification tasks. \\
    NN       & Neural Network                     & A model inspired by biological neural networks. \\
    API      & Application Programming Interface  & A set of rules allowing software components to communicate. \\
    GPU      & Graphics Processing Unit           & Often used to speed up training of ML models. \\
    JSON     & JavaScript Object Notation         & A lightweight data format often used for data exchange. \\
    UI       & User Interface                     & Visual elements users interact with. \\
    SDD      & Software Design Document           & A document that outlines the design of the software system. \\
    SRS      & Software Requirements Specification & A document that describes what the system should do. \\
    SHAP     & SHapley Additive exPlanations      & A method for explainable AI to attribute feature importance. \\
    DASH     & Dash by Plotly                     & Framework for building interactive dashboards in Python. \\
    \hline
    \end{tabular}
    \caption{Acronyms and Abbreviations}
\end{table}

\subsection{References}
\begin{itemize}
    \item IEEE 1016-2009 Software Design Description
    \item Any related SRS or technical documents
    \item \url{https://www.kaggle.com/competitions/spaceship-titanic/overview}
\end{itemize}

\subsection{Overview}
This Software Design Document (SDD) outlines the architecture and design of a machine learning system developed to predict which passengers aboard the Spaceship Titanic were transported to an alternate dimension following a spacetime anomaly.

The document is organized as follows:

\begin{itemize}
    \item \textbf{System Overview:} Provides a high-level summary of the system’s purpose and functionality.
    \item \textbf{Design Considerations:} Details assumptions, constraints, and development methodologies that influence the design.
    \item \textbf{Architectural Design:} Describes the overall structure of the system, including system components, data flow, and interfaces.
    \item \textbf{Detailed Design:} Explains the internal logic of major modules, including data preprocessing, model training, prediction, explainability, and dashboard.
    \item \textbf{Requirements Mapping:} Connects design elements back to the original project requirements.
    \item \textbf{Appendices:} Includes supporting materials such as a glossary, revision history, and additional diagrams.
\end{itemize}

This document is intended for developers, data scientists, stakeholders, and rescue mission coordinators who will use or maintain the system.

\section{System Overview}

The system is a machine learning-based application designed to assist in the recovery mission of the Spaceship Titanic by predicting which passengers were transported to an alternate dimension during the incident. Using data recovered from the ship's damaged computer systems—including demographic information, travel details, and activity logs—the AI model classifies passengers based on their likelihood of having been affected by the spacetime anomaly.

The core component of the system is a supervised learning model trained on labeled historical data. Once trained, the model can be deployed on a local computer or central server to analyze the entire passenger manifest and produce predictions in real-time or batch mode. 

The system pipeline includes the following stages:
\begin{itemize}
    \item \textbf{Data Ingestion:} Load and parse passenger data from CSV files.
    \item \textbf{Preprocessing:} Handle missing values, encode categorical variables, and normalize features.
    \item \textbf{Model Training:} Train a classification model (e.g., XGBoost, Random Forest, or Neural Network) using labeled records.
    \item \textbf{Prediction:} Generate predictions on unseen data to determine whether each passenger was transported.
    \item \textbf{Explainability:} Calculate SHAP values to interpret model outputs and generate feature importance summaries.
    \item \textbf{Dashboard:} Provide an interactive Dash-based dashboard for visualizing predictions, performance metrics, and SHAP explanations.
    \item \textbf{Output and Reporting:} Save predictions, SHAP reports, and dashboard snapshots for further analysis or integration with rescue operations.
\end{itemize}

The goal of this system is to maximize prediction accuracy while ensuring scalability, interpretability, and actionable insights through explainability and dashboard features.

\section{Design Considerations}

\subsection{Assumptions and Dependencies}
The following assumptions and dependencies have been identified during the design of the system:

\begin{itemize}
    \item \textbf{CVAT (Computer Vision Annotation Tool)} will be used to manually label and annotate any raw or semi-structured passenger data that requires supervised learning inputs. Labeled data will be exported in a format compatible with the training environment.
    
    \item \textbf{Google Colab} will be the primary environment for developing and training the AI model. It provides a cloud-based Jupyter notebook interface with access to GPUs, which significantly accelerates model training.
    
    \item \textbf{Python 3.x} will be the main programming language, with libraries such as \texttt{pandas}, \texttt{scikit-learn}, \texttt{xgboost}, \texttt{shap}, and \texttt{dash} used for data processing, model training, explainability, and dashboard generation.
    
    \item The recovered dataset from the Spaceship Titanic's computer systems is assumed to be mostly complete, with missing values handled during preprocessing.
    
    \item Final model artifacts (e.g., trained models in \texttt{.pkl} or \texttt{.h5} format) will be exported from Google Colab and run on a standard desktop or server for inference.
    
    \item It is assumed that there is no real-time data stream; predictions will be run in batch mode over the full dataset or user-specified subsets.
    
    \item Team members and operators interacting with the system will have basic familiarity with Python, Jupyter notebooks, and Dash.
    
    \item All environments must have consistent package versions to ensure reproducibility of training, inference, explainability, and dashboard rendering.
\end{itemize}

\subsection{General Constraints}
\begin{itemize}
    \item \textbf{Model Accuracy Limitation:} No machine learning model can guarantee 100\% prediction accuracy. The system is expected to make highly probable classifications, but some predictions may be incorrect due to limitations in the data and the nature of probabilistic modeling.
    
    \item \textbf{Hardware Constraints:} Model training and SHAP computation are constrained by the computational resources available in the free tier of Google Colab, including limited GPU/TPU availability, memory, and session duration. These limitations may impact training time, explainability runs, and dashboard refresh performance.
    
    \item \textbf{Data Privacy and Legal Compliance:} All processing of passenger data must comply with applicable interstellar data protection laws and ethical guidelines. Sensitive personal information must be handled with care, anonymized when appropriate, and not used in any way that violates privacy standards.
    
    \item \textbf{Data Quality Constraints:} The recovered passenger dataset may contain missing, inconsistent, or corrupted records due to damage sustained by the ship’s computer system. These limitations affect the quality and reliability of training, predictions, and explanations.
    
    \item \textbf{Dashboard Performance:} Rendering large datasets or complex SHAP visualizations may introduce latency. Dashboard pages will paginate, cache results, and allow asynchronous fetching to maintain responsiveness.
    
    \item \textbf{Offline Inference:} The trained model is expected to operate in an offline or batch inference mode. Real-time streaming predictions are not supported in the current design due to compute and data availability constraints.
    
    \item \textbf{Dependency Constraints:} The system depends on third-party tools such as CVAT for annotation, SHAP for explainability, and Dash for the dashboard. Any downtime or change in their APIs may impact the workflow.
\end{itemize}

\subsection{Goals and Guidelines}
\section{Goals}

The primary goal of this system is to identify and predict which passengers aboard the Spaceship Titanic were transported to an alternate dimension due to the spacetime anomaly. The system aims to aid rescue teams in efficiently prioritizing which passengers to attempt to retrieve from the alternate dimension.

Key goals of the system include:

\begin{itemize}
    \item \textbf{Prediction Accuracy:} To accurately predict which passengers were transported based on available data and the trained AI model. While the model cannot guarantee 100\% accuracy, it should provide a reliable and actionable prediction for rescue teams.
    
    \item \textbf{Explainability:} To provide clear, SHAP-based explanations that highlight the most influential features driving each prediction, aiding trust and decision-making.
    
    \item \textbf{Usability:} To deliver an interactive dashboard that allows users to explore predictions, metrics, and explanations without writing code.
    
    \item \textbf{Reliability:} The system should be robust, able to handle the full dataset, and provide consistent predictions and explanations with minimal downtime.
    
    \item \textbf{Timeliness:} Predictions, explanations, and dashboard updates should be generated quickly enough to support timely decision-making in the ongoing rescue operation.
    
    \item \textbf{Scalability:} The solution should be designed in a way that can easily scale to handle larger datasets, more complex models, or additional explainability methods in the future.
\end{itemize}

\subsection{Development Methods}
\section{DevOps Strategy}

The DevOps strategy for this AI-based project focuses on automation, collaboration, and continuous integration to ensure smooth development, testing, deployment, and maintenance of the system. Given the nature of the project, the system will rely on the following DevOps practices:

\subsection{Version Control}

\begin{itemize}
    \item \textbf{Git and GitHub/GitLab:} Version control will be managed using Git. All code, model and explainability scripts, and dashboard code will be stored in a Git repository, ensuring collaboration and version history tracking.
    \item \textbf{Branching Strategy:} A branching strategy such as Git Flow or feature-based branches will be used to manage development, with separate branches for data ingestion, modeling, explainability, dashboard, and production.
\end{itemize}

\subsection{Continuous Integration (CI)}

\begin{itemize}
    \item \textbf{GitHub Actions / GitLab CI:} Automated pipelines will be configured to run unit tests, linting, and validation checks on every commit. CI will include smoke tests for SHAP computation and dashboard rendering.
    \item \textbf{Test Automation:} Unit tests will cover data preprocessing, model training, SHAP value generation, and dashboard callbacks. Test coverage reports will be generated.
    \item \textbf{Pre-Commit Hooks:} Enforce code formatting (e.g., PEP8 for Python), notebook cleanliness, and dashboard layout consistency via linters.
\end{itemize}

\subsection{Continuous Deployment (CD)}

\begin{itemize}
    \item \textbf{Model & Explainability Artifacts:} Trained models, SHAP summary plots, and dashboard static assets will be versioned as CI artifacts.
    \item \textbf{Deployment Pipeline:} A Docker-based pipeline will package and deploy the model-serving API, SHAP report generator, and Dash app to production or staging servers.
    \item \textbf{Configuration Management:} Use external JSON config files for thresholds, file paths, model hyperparameters, and dashboard settings to allow runtime adjustments without code changes.
\end{itemize}

\subsection{Monitoring and Logging}

\begin{itemize}
    \item \textbf{Error Logging and Alerting:} Centralized logging (e.g., ELK stack) with Slack/email alerts on failures in data ingestion, model inference, SHAP generation, or dashboard rendering.
    \item \textbf{Performance Monitoring:} Track key metrics such as model latency, SHAP computation time, dashboard response times, and user engagement via analytics.
    \item \textbf{Retraining Trigger:} Automatically trigger retraining and SHAP regeneration pipelines when new data arrives or performance metrics degrade below predefined thresholds.
\end{itemize}

\subsection{Collaboration Tools}

\begin{itemize}
    \item \textbf{Slack / Teams:} Team communication will be facilitated via Slack or Microsoft Teams with dedicated channels for data, modeling, explainability, and dashboard discussions.
    \item \textbf{Jupyter Notebooks / Lab:} Google Colab and JupyterLab notebooks will be used for collaborative exploration of data, modeling experiments, and SHAP analyses.
\end{itemize}

\subsection{Backup and Data Storage}

\begin{itemize}
    \item \textbf{Cloud Storage / Git LFS:} Raw and processed datasets, model artifacts, and SHAP reports will be backed up to cloud storage (e.g., AWS S3, Google Drive) and large files managed via Git LFS.
    \item \textbf{Data Redundancy:} Maintain redundant copies of all critical data, model versions, and dashboard builds across multiple regions or providers.
\end{itemize}

\section{Architectural Design}

\subsection{System Architecture}
\section{System Architecture Design}

This section provides an overview of the system’s architecture, outlining the various components, how they interact, and the flow of data through the system. The architecture is designed to support the training, inference, explainability, and dashboard processes for predicting which passengers aboard the Spaceship Titanic were transported to an alternate dimension.

\subsection{High-Level Architecture}

The system is based on a layered architecture, with distinct layers for data ingestion, preprocessing, model training, prediction, explainability, dashboard, and output. The architecture is divided into the following key layers:

\begin{itemize}
    \item \textbf{Data Layer:} Collection and storage of raw and processed spaceship data.
    \item \textbf{Preprocessing Layer:} Data cleaning, imputation, encoding, and feature engineering.
    \item \textbf{Model Training Layer:} Training pipeline for XGBoost or alternative classifiers.
    \item \textbf{Inference Layer:} Batch inference service generating predictions.
    \item \textbf{Explainability Layer:} SHAP-based computation and report generation.
    \item \textbf{Dashboard Layer:} Dash app serving interactive visualizations and controls.
    \item \textbf{Output Layer:} Export of CSVs, SHAP reports, dashboard artifacts, and logs.
\end{itemize}

\subsection{Detailed Architecture}

The following diagram illustrates the system's architecture and the interactions between the various layers:

\begin{figure}[h!]
    \centering
    %\includegraphics[width=0.8\textwidth]{architecture-diagram.png}
    % WE NEED TO ADD ARCHITECTURE DIAGRAM LATER
    \caption{System Architecture Overview}
    \label{fig:architecture}
\end{figure}

The key components and data flow can be summarized as follows:

\begin{itemize}
    \item \textbf{Data Ingestion:} The spaceship’s recovered dataset is ingested into the system through manual file upload or automated scripts that pull data from a central repository (e.g., cloud storage or local file system).
    \item \textbf{Preprocessing:} Raw data is cleaned and preprocessed using Python scripts. Missing or inconsistent values are handled, and feature engineering is performed to prepare the dataset for training.
    \item \textbf{Model Training:} In Google Colab, we train the AI model on the preprocessed data using popular machine learning libraries (e.g., \texttt{scikit-learn}, \texttt{xgboost}, or \texttt{TensorFlow}). The trained model is exported for deployment.
    \item \textbf{Model Inference:} Once the model is deployed, it processes new data, making predictions on which passengers were transported to the alternate dimension. The model’s predictions are generated in batch mode and stored for analysis.
    \item \textbf{Explainability:} SHAP values are computed to generate global and local explanations; reports are produced in HTML format.
    \item \textbf{Dashboard:} A Dash app reads predictions and SHAP reports to display interactive visualizations and controls.
    \item \textbf{Output and Reporting:} The output predictions, SHAP HTML reports, and dashboard static build are saved and shared with rescue mission teams and stakeholders.
\end{itemize}

\subsection{Key Technologies Used}
\begin{itemize}
    \item \textbf{Google Colab:} Model training and SHAP computation environment with GPU/TPU support.
    \item \textbf{CVAT:} Annotation tool for data labeling.
    \item \textbf{Python 3.x:} Core language for pipelines, explainability, and dashboard.
    \item \texttt{scikit-learn}, \texttt{xgboost}: Modeling libraries.
    \item \texttt{shap}: Explainability library.
    \item \texttt{dash}, \texttt{plotly}: Dashboard framework.
    \item \textbf{Docker:} Containerization for services.
    \item \textbf{JSON:} Configuration management.
    \item \textbf{Slack / Email:} Alerting and collaboration.
\end{itemize}

\section{Component Design}

\subsection{AI Model Component}

The AI model used in this project is based on the **XGBoost (Extreme Gradient Boosting)** algorithm, a highly efficient and scalable implementation of gradient boosting. XGBoost is particularly suitable for structured data, like the passenger data, and can handle a large number of features effectively.

\subsubsection{XGBoost Algorithm}

XGBoost works by combining the predictions of many base learners (decision trees), with each successive model trained to correct the errors of its predecessors. The primary mechanism behind the algorithm is **gradient boosting**, where models are trained in a stage-wise manner. At each stage, the model minimizes the residuals (errors) from the previous stage.

\begin{itemize}
    \item \textbf{Objective Function:} The objective function is the sum of a loss function (such as log loss or mean squared error) and a regularization term that penalizes complex models to avoid overfitting.
    \item \textbf{Base Learner:} The base learner is a decision tree, which is optimized at each stage to minimize the loss.
    \item \textbf{Boosting Strategy:} Trees are added sequentially, with each new tree aimed at reducing the residual errors of previous ones.
\end{itemize}

\subsubsection{Key Hyperparameters}

The key hyperparameters for tuning the XGBoost model include:

\begin{itemize}
    \item \textbf{Learning Rate (eta):} Controls the contribution of each tree to the final model. A lower learning rate generally leads to better generalization but requires more trees.
    \item \textbf{Number of Trees (n\_estimators):} The total number of trees to be added in the boosting process.
    \item \textbf{Maximum Depth (max\_depth):} The maximum depth of the decision trees. This controls the complexity of each individual tree.
    \item \textbf{Subsample:} The fraction of samples to be used for training each tree. This helps prevent overfitting.
    \item \textbf{Colsample\_bytree:} The fraction of features to be used for each tree.
\end{itemize}

\subsection{Preprocessing Component}

Before the data is fed into the model, it undergoes preprocessing to ensure it is clean and ready for training.

\begin{itemize}
    \item \textbf{Handling Missing Values:} Any missing data in the dataset is handled by imputation, either using the mean, median, or a model-based approach depending on the feature type.
    \item \textbf{Feature Scaling:} Continuous variables are scaled to ensure that no feature dominates the others, especially for distance-based algorithms. However, XGBoost is generally less sensitive to feature scaling.
    \item \textbf{Feature Encoding:} Categorical variables are encoded into numerical values using techniques such as one-hot encoding or label encoding, depending on the type of categorical feature.
    \item \textbf{Outlier Removal:} Any extreme outliers are identified and either removed or adjusted to prevent them from skewing the model.
\end{itemize}

\subsection{Model Training Component}

The model training component focuses on training the XGBoost model on the preprocessed data.

\subsubsection{Optimizer:}

XGBoost uses **Gradient Descent** as the optimizer, which is applied to minimize the objective function (the loss function plus regularization). The optimization process is done iteratively, with each decision tree trained to minimize residual errors from the previous stage.

\begin{itemize}
    \item \textbf{Objective Function:} For binary classification tasks (like this one, where the task is to predict whether a passenger was transported to another dimension), we will use the **log loss** function, which is commonly used for binary classification problems.
    \item \textbf{Regularization:} To avoid overfitting, XGBoost includes a regularization term in the objective function, penalizing more complex models.
    \item \textbf{Learning Rate:} The learning rate determines how quickly the model converges. A lower rate typically leads to more accurate results but requires more iterations.
\end{itemize}

\subsubsection{Training Process:}

The training process will include the following steps:
\begin{itemize}
    \item Split the dataset into training and validation sets (10\% training, 10\% validation).
    \item Train the model using the training data, iterating over the dataset multiple times (epochs) and adjusting the model parameters to minimize the objective function.
    \item Monitor the model’s performance on the validation set during training to prevent overfitting.
\end{itemize}

\subsection{Model Evaluation Component}

After training, the model is evaluated using appropriate metrics to assess its accuracy and generalization ability.

\begin{itemize}
    \item \textbf{Accuracy:} The percentage of correct predictions (i.e., the number of true positives and true negatives divided by the total number of predictions).
    \item \textbf{Confusion Matrix:} To understand the performance in more detail, especially for binary classification, a confusion matrix will be used to show the counts of true positives, false positives, true negatives, and false negatives.
    \item \textbf{ROC-AUC Score:} The area under the receiver operating characteristic (ROC) curve is a performance measurement for classification problems. A higher AUC score indicates a better-performing model.
\end{itemize}

\subsection{Prediction Module}

\begin{itemize}
    \item \textbf{Description:} This module takes the trained XGBoost model and uses it to make predictions for incoming passenger data. The model's output is a probability that the passenger has been transported to the alternate dimension.
    \item \textbf{Inputs/Outputs:} 
    \begin{itemize}
        \item Inputs: New passenger data (age, gender, etc.).
        \item Outputs: Prediction probability of whether the passenger was transported or not, along with a binary label based on a configurable threshold.
    \end{itemize}
    \item \textbf{Algorithms or Logic:} 
    \begin{itemize}
        \item The prediction is made by passing the new passenger data through the trained XGBoost model. The output is a probability, and a threshold (set in the JSON configuration file) is applied to classify the passenger as either "transported" or "not transported."
    \end{itemize}
\end{itemize}

\subsection{User Interface Module}

\begin{itemize}
    \item \textbf{Description:} This module handles the web-based user interface that allows users to input data and receive predictions.
    \item \textbf{Inputs/Outputs:} 
    \begin{itemize}
        \item Inputs: Passenger data input through web forms (age, gender, etc.).
        \item Outputs: Prediction results displayed on the web interface, including a confidence score and link to SHAP explanation.
    \end{itemize}
    \item \textbf{Algorithms or Logic:} 
    \begin{itemize}
        \item The form input is sent to the backend via an HTTP request. The backend uses the trained model to make predictions and sends the result back to the frontend for display, along with a link to the corresponding SHAP local explanation.
    \end{itemize}
\end{itemize}

\section{Explainability Module}

\begin{itemize}
    \item \textbf{Description:} Computes SHAP values for model interpretability and generates global and local explanation reports.
    \item \textbf{Inputs/Outputs:}
    \begin{itemize}
        \item Inputs: Trained model, sample features dataset.
        \item Outputs: SHAP summary plots, local force-plots, and a consolidated HTML report.
    \end{itemize}
    \item \textbf{Algorithms or Logic:}
    \begin{enumerate}
        \item Load trained model and representative sample data.
        \item Use SHAP’s TreeExplainer to calculate SHAP values.
        \item Generate global feature importance summary plot.
        \item Generate local force-plots for selected high-risk passengers.
        \item Export all visualizations into an HTML report.
    \end{enumerate}
    \item \textbf{Libraries:} \texttt{shap}, \texttt{matplotlib}.
\end{itemize}

\section{Dashboard Module}

\begin{itemize}
    \item \textbf{Description:} An interactive Dash-based web dashboard for exploring model predictions, performance metrics, and SHAP explanations.
    \item \textbf{Inputs/Outputs:}
    \begin{itemize}
        \item Inputs: Predictions CSV, SHAP HTML report paths, performance metrics JSON.
        \item Outputs: Live dashboard accessible at a specified URL; exportable CSV and image snapshots.
    \end{itemize}
    \item \textbf{Logic:}
    \begin{enumerate}
        \item Load predictions and metrics into pandas DataFrames.
        \item Initialize Dash layout with:
        \begin{itemize}
            \item Summary cards (accuracy, precision, recall, F1 score).
            \item Time-series chart of batch run metrics.
            \item Filterable, paginated data table of individual predictions.
            \item Embedded SHAP global summary plot.
            \item Control widgets (threshold slider, date picker) that trigger callbacks.
        \end{itemize}
        \item Define callbacks to:
        \begin{itemize}
            \item Update charts and tables based on widget inputs.
            \item Toggle visibility of SHAP force-plots.
            \item Export the current view as CSV or PNG.
        \end{itemize}
        \item Cache heavy computations using \texttt{flask\_caching} for performance.
    \end{enumerate}
    \item \textbf{Libraries:} \texttt{dash}, \texttt{plotly}, \texttt{pandas}, \texttt{flask\_caching}.
    \item \textbf{UI Layout:} Responsive grid with cards at top, graphs in middle, tables at bottom; collapsible sections for mobile.
\end{itemize}

\section{Requirements Mapping}
This section traces the design elements back to the requirements defined in the Software Requirements Specification (SRS).

\begin{itemize}
    \item The training module fulfills the requirement to predict whether a passenger was transported.
    \item The prediction module ensures batch predictions with configurable thresholds.
    \item The explainability module meets the requirement for model interpretability via SHAP.
    \item The dashboard module satisfies the requirement for an interactive user interface.
    \item The configuration file support allows runtime adjustments without code changes.
    \item The monitoring and logging strategy meets operational reliability requirements.
\end{itemize}

\section{Appendices}

\subsection{Appendix A: Glossary}

\begin{itemize}
    \item \textbf{SHAP:} SHapley Additive exPlanations, a framework for interpretable machine learning.
    \item \textbf{Dash:} A Python framework by Plotly for building interactive web dashboards.
    \item \textbf{Force-Plot:} A local explanation visualization showing feature contributions for a single prediction.
\end{itemize}

\subsection{Appendix B: Revision History}
\begin{tabular}{|l|l|l|l|}
\hline
Version & Date & Author & Description \\
\hline
1.0 & \today & Jasmine Pineda & Initial draft \\
\hline
1.1 & \today & Braedon Edison & Added configuration file support and error-logging/alerting module \\
\hline
1.2 & \today & Braedon Edison & Added SHAP explainability and dashboard features \\
\hline
\end{tabular}

\end{document}
